{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-q3TC33FBPO",
        "outputId": "d6d58770-1a5c-4570-87ec-4120e3cc6bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent did not reach the goal.\n",
            "Final Q-table:\n",
            "[[[  0.           0.           0.           0.        ]\n",
            "  [  0.         -10.           0.           0.        ]\n",
            "  [  0.           0.           0.           0.        ]\n",
            "  [  0.           0.           0.           0.        ]\n",
            "  [  0.           0.           0.           0.        ]]\n",
            "\n",
            " [[  0.           0.           0.          -9.41850263]\n",
            "  [  0.           0.           0.           0.        ]\n",
            "  [  0.          -5.6953279   -1.9          0.        ]\n",
            "  [  0.           0.           0.           0.        ]\n",
            "  [  0.           0.           0.           0.        ]]\n",
            "\n",
            " [[  0.           0.           0.           0.        ]\n",
            "  [ -1.           0.           0.           0.        ]\n",
            "  [  0.           0.           0.           0.        ]\n",
            "  [  0.           0.           0.           0.        ]\n",
            "  [  0.           0.           0.           0.        ]]\n",
            "\n",
            " [[  0.           0.           0.           0.        ]\n",
            "  [  0.           0.           0.           0.        ]\n",
            "  [  0.           0.           0.           0.        ]\n",
            "  [  0.           0.           0.           0.        ]\n",
            "  [  0.           0.           0.           0.        ]]\n",
            "\n",
            " [[  0.           0.           0.           0.        ]\n",
            "  [  0.           0.           0.           0.        ]\n",
            "  [  0.           0.           0.           0.        ]\n",
            "  [  0.           0.           0.           0.        ]\n",
            "  [  0.           0.           0.           0.        ]]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the grid world environment\n",
        "grid_size = (5, 5)  # Size of the grid world\n",
        "start_state = (0, 0)  # Starting state\n",
        "goal_state = (4, 4)  # Goal state\n",
        "obstacle_states = [(1, 1), (2, 2), (3, 3)]  # Obstacle states\n",
        "\n",
        "# Define the action space\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "\n",
        "# Initialize the Q-table\n",
        "q_table = np.zeros((grid_size[0], grid_size[1], len(actions)))\n",
        "\n",
        "# Set hyperparameters\n",
        "learning_rate = 0.1\n",
        "discount_factor = 0.9\n",
        "num_episodes = 1000\n",
        "max_steps_per_episode = 100\n",
        "\n",
        "# Q-learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "    state = start_state\n",
        "    for step in range(max_steps_per_episode):\n",
        "        # Choose an action using epsilon-greedy policy\n",
        "        if np.random.uniform() < 0.1:\n",
        "            action = np.random.choice(actions)\n",
        "        else:\n",
        "            action = actions[np.argmax(q_table[state])]\n",
        "        \n",
        "        # Perform the action and observe the next state and reward\n",
        "        if action == 'up' and state[0] > 0:\n",
        "            next_state = (state[0] - 1, state[1])\n",
        "        elif action == 'down' and state[0] < grid_size[0] - 1:\n",
        "            next_state = (state[0] + 1, state[1])\n",
        "        elif action == 'left' and state[1] > 0:\n",
        "            next_state = (state[0], state[1] - 1)\n",
        "        elif action == 'right' and state[1] < grid_size[1] - 1:\n",
        "            next_state = (state[0], state[1] + 1)\n",
        "        else:\n",
        "            next_state = state\n",
        "        \n",
        "        if next_state in obstacle_states:\n",
        "            reward = -10\n",
        "        elif next_state == goal_state:\n",
        "            reward = 10\n",
        "        else:\n",
        "            reward = 0\n",
        "        \n",
        "        # Update the Q-value for the current state-action pair\n",
        "        q_table[state][actions.index(action)] += learning_rate * (\n",
        "            reward + discount_factor * np.max(q_table[next_state]) - q_table[state][actions.index(action)]\n",
        "        )\n",
        "        \n",
        "        state = next_state\n",
        "        \n",
        "        if state == goal_state or state in obstacle_states:\n",
        "            break\n",
        "\n",
        "# Evaluate the learned policy\n",
        "state = start_state\n",
        "steps = 0\n",
        "while state != goal_state and steps < max_steps_per_episode:\n",
        "    action = actions[np.argmax(q_table[state])]\n",
        "    \n",
        "    if action == 'up' and state[0] > 0:\n",
        "        next_state = (state[0] - 1, state[1])\n",
        "    elif action == 'down' and state[0] < grid_size[0] - 1:\n",
        "        next_state = (state[0] + 1, state[1])\n",
        "    elif action == 'left' and state[1] > 0:\n",
        "        next_state = (state[0], state[1] - 1)\n",
        "    elif action == 'right' and state[1] < grid_size[1] - 1:\n",
        "        next_state = (state[0], state[1] + 1)\n",
        "    else:\n",
        "        next_state = state\n",
        "    \n",
        "    state = next_state\n",
        "    steps += 1\n",
        "\n",
        "# Print the final path taken by the agent\n",
        "if state == goal_state:\n",
        "    print(\"Goal reached!\")\n",
        "else:\n",
        "    print(\"Agent did not reach the goal.\")\n",
        "\n",
        "# Print the final Q-table\n",
        "print(\"Final Q-table:\")\n",
        "print(q_table)\n"
      ]
    }
  ]
}